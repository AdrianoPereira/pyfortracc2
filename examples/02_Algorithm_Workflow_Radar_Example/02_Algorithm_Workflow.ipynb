{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"; span style=\"color:#336699\"><b><h2> Algorithm Workflow </h2></b></div>\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "<br/>\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    Helvécio B. Leal Neto, <sup><a href=\"https://orcid.org/0000-0002-7526-2094\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    Alan J. P. Calheiros<sup><a href=\"https://orcid.org/0000-0002-7526-2094\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    <br/><br/>\n",
    "    National Institute for Space Research (INPE)\n",
    "    <br/>\n",
    "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
    "    <br/><br/>\n",
    "    Contact: <a href=\"mailto:helvecio.neto@inpe.br\">helvecio.neto@inpe.br</a>, <a href=\"mailto:alan.calheiros@inpe.br\">alan.calheiros@inpe.br</a>\n",
    "    <br/><br/>\n",
    "    Last Update: Jun 16, 2024\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
    "<b>Abstract.</b> This Jupyter notebook demonstrates the processing flow of the pyForTraCC algorithm. There you can check the example of tracking using radar data.\n",
    "</div>    \n",
    "<br/>\n",
    "<div style=\"text-align: justify;  margin-left: 15%; margin-right: 15%;font-size: 75%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>This Jupyter Notebook was based on the <a href=\"https://github.com/fortracc-project/pyfortracc\">\"pyfortracc\"</a> example from the pyFortracc:</b>\n",
    "    <div style=\"margin-left: 10px; margin-right: 10px; margin-top:10px\">\n",
    "      <p> Leal Neto, H.B.; Calheiros, A.J.P.;  pyForTraCC Algorithm. São José dos Campos, INPE, 2024. <a href=\"https://github.com/fortracc-project/pyfortracc\" target=\"_blank\"> Online </a>. </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule\n",
    "\n",
    " [1. Radar Data](#data)<br>\n",
    " [2. Setting the Environment](#install)<br>\n",
    " [3. Read Function](#read)<br>\n",
    " [4. Tracking Parameters](#parameters)<br>\n",
    " [5. Algorithm Workflow](#parameters)<br>\n",
    " [- Features Extraction](#features)<br>\n",
    " [- Spatial Operations](#spatial)<br>\n",
    " [- Cluster Link](#link)<br>\n",
    " [6. Explore Output](#output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "#### 1. Radar Data\n",
    "\n",
    "The data in this example corresponds to a small sample of scans from the S-Band Radar located in the city of Manaus-AM Brazil.<br>\n",
    " Data processed and published by Schumacher, Courtney and Funk, Aaron (2018) were separated, <br>\n",
    " and are available in full on the ARM platform https://www.arm.gov/research/campaigns/amf2014goamazon.<br>\n",
    " This data is part of the GoAmazon2014/5 project and is named \"Three-dimensional Gridded S-band Reflectivity and Radial Velocity<br>\n",
    " from the SIPAM Manaus S-band Radar dataset\".<br>\n",
    "https://doi.org/10.5439/1459573\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the input files\n",
    "!pip install --upgrade --no-cache-dir gdown &> /dev/null\n",
    "!gdown 'https://drive.google.com/uc?id=1UVVsLCNnsmk7_wOzVrv4H7WHW0sz8spg'\n",
    "!unzip -qq -o input.zip\n",
    "!rm -rf input.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='install'></a>\n",
    "#### 2. Setting the environment\n",
    "\n",
    "Install package to environment and import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install the latest version of pyfortracc from the main branch\n",
    "# !pip install --upgrade git+https://github.com/fortracc-project/pyfortracc.git@main#egg=pyfortracc\n",
    "# Or import the local version of pyfortracc\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyfortracc package\n",
    "import pyfortracc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='read'></a>\n",
    "#### 3. Read Function\n",
    "\n",
    "The downloaded data is compressed with the .gz extension, however, it is of the netCDF4 type. The variable that represents reflectivity is DBZc. This data also has multiple elevations, and we arbitrarily chose elevation 5, which corresponds to the volumetric scan level at 2.5 km height. We extract the data and apply a NaN value to the data mask -9999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Read function\n",
    "import gzip\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "def read_function(path):\n",
    "    variable = \"DBZc\"\n",
    "    z_level = 5 # Elevation level 2.5 km\n",
    "    with gzip.open(path) as gz:\n",
    "        with netCDF4.Dataset(\"dummy\", mode=\"r\", memory=gz.read()) as nc:\n",
    "            data = nc.variables[variable][:].data[0,z_level, :, :][::-1, :]\n",
    "            data[data == -9999] = np.nan\n",
    "    gz.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "pyfortracc.plot_animation(path_files='input/*.gz', \n",
    "                          num_frames=10, figsize=(4, 4), cbar_min=-10, cbar_max=60,\n",
    "                          read_function=read_function,  cbar_title='dBZ', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parameters'></a>\n",
    "#### 4. Tracking Parameters\n",
    "\n",
    "For this example, we will track reflectivity clusters at multiple thresholds and sizes <br>Arbitrarily selecting thresholds of 20, 30 and 35 dBZ with clusters of 5,4 and 3 pixels <br>of minimum size. The segmentation operator will be >=, that is, the clusters will be <br>segmented in order of greatest equal for each threshold and the delta of the observations <br>is 12 minutes. The clustering algorithm in this example will be DBSCAN and the overlap <br>rate between clusters of consecutive times will be 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Name list dictionary of mandatory parameters\n",
    "name_list = {}\n",
    "name_list['input_path'] = 'input/' # path to the input data\n",
    "name_list['output_path'] = 'output/' # path to the output data\n",
    "name_list['timestamp_pattern'] = 'sbmn_cappi_%Y%m%d_%H%M.nc.gz' # timestamp file pattern\n",
    "name_list['thresholds'] = [20,30,35] # in dbz\n",
    "name_list['min_cluster_size'] = [3,3,3] # in number of points per cluster\n",
    "name_list['operator'] = '>=' # '>= - <=' or '=='\n",
    "name_list['delta_time'] = 12 # in minutes\n",
    "name_list['cluster_method'] = 'dbscan' # DBSCAN Clustering method\n",
    "name_list['min_overlap'] = 20 # Minimum overlap between clusters in percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='workflow'></a>\n",
    "#### 5. Algorithm Workflow\n",
    "\n",
    "In this example we will use the modules separately, that is, each internal module of the pyForTraCC algorithm will be called individually.<br>\n",
    "The track workflow is divided into four modules:\n",
    "<ol>\n",
    "  <li><b>Feature detection</b>: Focuses on identifying distinct characteristics for precise tracking.\n",
    "  </li>\n",
    "  <li><b>Spatial Operations</b>: Involves spatial operations (intersection, union, difference, etc) between the features of consecutive time steps (t and t-1).\n",
    "  <li><b>Trajectory Linking</b>: Passing one by one the time steps, the algorithm link the features of consecutive time steps based on the association created in the previous step. The algorithm create a trajectory for each feature.\n",
    "  <li><b>Concatenate</b>:\n",
    "  </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Features Extraction\n",
    "Image processing strategies were combined with clustering and rasterization algorithms to achieve the results obtained by the extract_features function. The following sequence is present in the implementation of the algorithm:\n",
    "\n",
    "1. Read the file using the read_funcion.\n",
    "2. Image segmentation according to each threshold.\n",
    "3. Labeling of clusters. Two clustering options are implemented (DBSCAN and ndimage). \n",
    "5. Vectorization of the clusters using rasterio.features.shapes to acquire the boundary polygon and the centroid of the clusters.\n",
    "\n",
    "<span> <img src=\"./img/features.png\" style=\"height:320px;\" align=\"left\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Note: If you are running this notebook using MacOS, you may need to set parallel=False)\n",
    "pyfortracc.features_extraction(name_list, read_function, parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "features = sorted(glob.glob(name_list['output_path'] + '/track/processing/features/*.parquet'))\n",
    "features = pd.concat(pd.read_parquet(f) for f in features)\n",
    "display(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Spatial operations \n",
    "\n",
    "Spatial junctions, fundamental in Geographic Information Systems (GIS) and spatial databases. And the Geopandas implementation simplifies the process by combining GeoDataframes stored in .parquet files based on their spatial relationships via the sjoin() method. This method performs various types of spatial joins such as overlays, within, contains.\n",
    "\n",
    "To demonstrate the basic functioning of the spatial operations mode, we will use as a base two consecutive times listed here as the variables cur_frame and prev_frame, which store information about the characteristics of the systems for each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.wkt import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the features parquet files\n",
    "cur_frame = pd.read_parquet('./output/track/processing/features/20140816_1012.parquet')\n",
    "prev_frame = pd.read_parquet('./output/track/processing/features/20140816_1000.parquet')\n",
    "cur_frame['geometry'] = cur_frame['geometry'].apply(loads)\n",
    "prev_frame['geometry'] = prev_frame['geometry'].apply(loads)\n",
    "# Convert to geo dataframes where column is geometry\n",
    "cur_frame = gpd.GeoDataFrame(cur_frame)\n",
    "prev_frame = gpd.GeoDataFrame(prev_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the features and see have a visual overlap between the geometries\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "prev_frame.boundary.plot(ax=ax, color='blue', alpha=0.5)\n",
    "cur_frame.plot(ax=ax, color='red', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the current frame\n",
    "display(cur_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the previous frame\n",
    "display(prev_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate one of the operations performed in the algorithm, below is the GeoPandas sjoin function that checks the overlaps between the two GeoDataframes. \n",
    "Note that the return of the function will be another GeoDataframe, however only the index and index_right columns are of interest to us, \n",
    "as in these columns we have the information we need to make the associations between the geometries of the consecutive time clusters.\n",
    "In addition to the overlap operation, there are several others that can be seen at:\n",
    "https://shapely.readthedocs.io/en/latest/manual.html#binary-predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the spatial join between the previous and current frame\n",
    "overlaps = gpd.sjoin(cur_frame, prev_frame, how='inner', predicate='overlaps')[['index_right']].reset_index()\n",
    "# Ex: index (cur_frame) 0 overlaps with index_right 0 (prev_franme)\n",
    "overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the spatial operations module also has additional vector extraction methods. These methods are covered in the work https://doi.org/10.3390/rs14215408\n",
    "\n",
    "To activate the methods just add the flags to the name_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add correction methods\n",
    "name_list['spl_correction'] = True # It is used to perform the correction at Splitting events\n",
    "name_list['mrg_correction'] = True # It is used to perform the correction at Merging events\n",
    "name_list['inc_correction'] = True # It is used to perform the correction using Inner Core vectors\n",
    "name_list['opt_correction'] = True # It is used to perform the correction using the Optical Flow method\n",
    "name_list['validation'] = True # It is used to perform the validation of the correction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the spatial_operations function (Note: If you are running this notebook using MacOS, you may need to set parallel=False)\n",
    "pyfortracc.spatial_operations(name_list, read_function, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the spatial parquet files\n",
    "spatial = sorted(glob.glob(name_list['output_path'] + '/track/processing/features/*.parquet'))\n",
    "spatial_df = pd.concat(pd.read_parquet(f) for f in spatial)\n",
    "display(spatial_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster Link\n",
    "\n",
    "The cluster connection module makes the association between consecutive time tables by associating the cluster indices that were identified by the spatial operations module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cluster_linking function\n",
    "pyfortracc.cluster_linking(name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we show how cluster link works. Set two spatial parquets from consecutive timestamps. And show the association between them based on indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read current and previous frames\n",
    "cur_frame = pd.read_parquet('output/track/processing/spatial/20140816_1212.parquet')\n",
    "prv_frame = pd.read_parquet('output/track/processing/spatial/20140816_1200.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Frame\n",
    "cur_frame.dropna(subset=['prev_idx']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous frame\n",
    "prv_frame.loc[cur_frame['prev_idx'].dropna().astype(int).values].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the linking results\n",
    "linked = sorted(glob.glob(name_list['output_path'] + '/track/processing/linked/*.parquet'))\n",
    "linking_df = pd.concat(pd.read_parquet(f) for f in linked)\n",
    "linking_df.loc[linking_df['trajectory'] != 'LINESTRING EMPTY'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate \n",
    "\n",
    "All features in one single file per timestamp, and create the Tracking Table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the features and spatial dataframes\n",
    "pyfortracc.concat(name_list, clean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tracking table is the generalized output entity of the algorithm, it is formed by the set of files (.parquet) that are located in the output directory of the same name ('output_path/trackingtable'). The information obtained in the tracking process is stored in a tabular format, and is organized according to the tracking time. Listed below are the names of the columns (output variables) and what they represent.\n",
    "\n",
    "- Each row in the tracking specific data related to a cluster at its corresponding threshold level. \n",
    "- The information spans from unique identifiers and descriptive statistics to geometric properties and temporal features. \n",
    "- The Tracking Table structure provides a comprehensive view of grouped entities, facilitating analysis and understanding of patterns across different threshold levels.\n",
    "\n",
    "Tracking table columns:\n",
    "\n",
    "*   **timestamp** (datetime64[us]): Temporal information of cluster.\n",
    "*   **uid** (float64): Unique idetifier of cluster.\n",
    "*   **iuid** (float64): Internal Unique idetifier of cluster.\n",
    "*   **threshold_level** (int64): Level of threshold.\n",
    "*   **threshold** (float64): Specific threshold.\n",
    "*   **lifetime** (timedelta64[ns]): Cluster lifespan.\n",
    "*   **status** (object): Entity status (NEW, CONTINUOUS, SPLIT, MERGE, SPLIT/MERGE)\n",
    "*   **u_, v_** (float64): Vector components.\n",
    "*   **inside_clusters** (object): Number of inside clusters.\n",
    "*   **size** (int64): Cluster size in pixels.\n",
    "*   **min, mean, max, std, Q1, Q2, Q3** (float64): Descriptive statistics.\n",
    "*   **delta_time** (timedelta64[us]): Temporal variation.\n",
    "*   **file** (object): Associated file name.\n",
    "*   **array_y, array_x** (object): Cluster array coordinates.\n",
    "*   **vector_field** (object): Associated vector field.\n",
    "*   **trajectory** (object): Cluster's trajectory.\n",
    "*   **geometry** (object):  Boundary geometric representation of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_files = sorted(glob.glob(name_list['output_path'] + '/track/trackingtable/*.parquet'))\n",
    "tracking_table = pd.concat(pd.read_parquet(f) for f in tracking_files)\n",
    "display(tracking_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If need to save the family table into separate files or unique file run the cell below\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "tracking_files = sorted(glob.glob(name_list['output_path'] + '/track/trackingtable/*.parquet'))\n",
    "tracking_table = pd.concat(pd.read_parquet(f) for f in tracking_files)\n",
    "\n",
    "family_group = tracking_table.groupby('uid')\n",
    "family_table = pd.DataFrame()\n",
    "pathlib.Path('output/track/trackingtable/family').mkdir(parents=True, exist_ok=True)\n",
    "for _, group in family_group:\n",
    "    family_table = pd.concat([family_table, group])\n",
    "    # # If need save in separate files uncomment the line below\n",
    "    # uid_ = group['uid'].iloc[0]\n",
    "    # group.to_csv(f'output/track/trackingtable/family/{uid_}.csv')\n",
    "# If need save into unique file uncomment the line below\n",
    "family_table.to_csv('output/track/trackingtable/family.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the results in tracking table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get two maxlifetime clusters from the track_table\n",
    "maxlifetime = 2\n",
    "max_lifetimes = tracking_table.groupby('uid').size().nlargest(maxlifetime).index.values\n",
    "max_clusters = tracking_table[tracking_table['uid'].isin(max_lifetimes)]\n",
    "max_clusters.loc[max_clusters['threshold_level'] == 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the track as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add plot the tracking data\n",
    "name_list['lon_min'] = -62.1475 # Min longitude of data in degrees\n",
    "name_list['lon_max'] = -57.8461 # Max longitude of data in degrees\n",
    "name_list['lat_min'] = -5.3048 # Min latitude of data in degrees\n",
    "name_list['lat_max'] = -0.9912 # Max latitude of data in degrees\n",
    "name_list['x_dim'] = 241 # Number of points in the x axis\n",
    "name_list['y_dim'] = 241 # Number of points in the y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tracking data for periods of time\n",
    "pyfortracc.plot_animation(read_function=read_function, name_list=name_list, \n",
    "                          figsize=(14,5), cbar_title='dBZ', vector=True, vector_scale=10,\n",
    "                          threshold_list=[20], uid_list=max_lifetimes.tolist(),\n",
    "                          info=True, info_col_name=True, info_cols=['uid', 'method', 'far'],\n",
    "                          smooth_trajectory=True,\n",
    "                          start_stamp = '2014-08-16 12:24:00', \n",
    "                          end_stamp='2014-08-16 20:48:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the results to spatial data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyfortracc.spatial_conversions(name_list, boundary=True, trajectory=True, vector_field=True, cluster=True, vel_unit='m/s', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrotrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
