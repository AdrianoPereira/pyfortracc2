{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"; span style=\"color:#336699\"><b><h2> Algorithm Workflow </h2></b></div>\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "<br/>\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    Helvécio B. Leal Neto, <sup><a href=\"https://orcid.org/0000-0002-7526-2094\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    Alan J. P. Calheiros<sup><a href=\"https://orcid.org/0000-0002-7526-2094\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    <br/><br/>\n",
    "    National Institute for Space Research (INPE)\n",
    "    <br/>\n",
    "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
    "    <br/><br/>\n",
    "    Contact: <a href=\"mailto:helvecio.neto@inpe.br\">helvecio.neto@inpe.br</a>, <a href=\"mailto:alan.calheiros@inpe.br\">alan.calheiros@inpe.br</a>\n",
    "    <br/><br/>\n",
    "    Last Update: Nov 1, 2024\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
    "<b>Abstract.</b> This Jupyter notebook demonstrates the complete processing flow of the tracking process of pyForTraCC algorithm. There you can check the example of tracking using radar data.\n",
    "</div>    \n",
    "<br/>\n",
    "<div style=\"text-align: justify;  margin-left: 15%; margin-right: 15%;font-size: 75%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>This Jupyter Notebook was based on the <a href=\"https://github.com/fortracc-project/pyfortracc\">\"pyfortracc\"</a> example from the pyFortracc:</b>\n",
    "    <div style=\"margin-left: 10px; margin-right: 10px; margin-top:10px\">\n",
    "      <p> Leal Neto, H.B.; Calheiros, A.J.P.;  pyForTraCC Algorithm. São José dos Campos, INPE, 2024. <a href=\"https://github.com/fortracc-project/pyfortracc\" target=\"_blank\"> Online </a>. </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule\n",
    "\n",
    " [1. Installation](#install)<br>\n",
    " [2. Radar Input Data](#data)<br>\n",
    " [3. Read Function](#read)<br>\n",
    " [4. Tracking Parameters](#parameters)<br>\n",
    " [5. Algorithm Workflow](#parameters)<br>\n",
    " [- Features Extraction](#features)<br>\n",
    " [- Spatial Operations](#spatial)<br>\n",
    " [- Cluster Link](#link)<br>\n",
    " [6. Explore Output](#output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='install'></a>\n",
    "#### 1. Installation\n",
    "\n",
    "Installing the pyFortraCC package can be done using the pip install command. \n",
    "\n",
    "All dependencies will be installed in the current Python environment and the code will be ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install directly from github\n",
    "!pip3 install --upgrade git+https://github.com/fortracc/pyfortracc.git@main#egg=pyfortracc &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "#### 1. Radar Input Data\n",
    "\n",
    "The data in this example corresponds to a small sample of scans from the S-Band Radar located in the city of Manaus-AM Brazil.<br>\n",
    " Data processed and published by Schumacher, Courtney and Funk, Aaron (2018) were separated, <br>\n",
    " and are available in full on the ARM platform https://www.arm.gov/research/campaigns/amf2014goamazon.<br>\n",
    " This data is part of the GoAmazon2014/5 project and is named \"Three-dimensional Gridded S-band Reflectivity and Radial Velocity<br>\n",
    " from the SIPAM Manaus S-band Radar dataset\".<br>\n",
    "https://doi.org/10.5439/1459573\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the exemple input files\n",
    "!pip3 install --upgrade --no-cache-dir gdown &> /dev/null\n",
    "!gdown 'https://drive.google.com/uc?id=1UVVsLCNnsmk7_wOzVrv4H7WHW0sz8spg'\n",
    "!unzip -qq -o input.zip\n",
    "!rm -rf input.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='read'></a>\n",
    "#### 3. Read Function\n",
    "\n",
    "The downloaded data is compressed with the .gz extension, however, it is of the netCDF4 type. The variable that represents reflectivity is DBZc. This data also has multiple elevations, and we arbitrarily chose elevation 5, which corresponds to the volumetric scan level at 2.5 km height. We extract the data and apply a NaN value to the data mask -9999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Read function\n",
    "import gzip\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "def read_function(path):\n",
    "    variable = \"DBZc\"\n",
    "    z_level = 5 # Elevation level 2.5 km\n",
    "    with gzip.open(path) as gz:\n",
    "        with netCDF4.Dataset(\"dummy\", mode=\"r\", memory=gz.read()) as nc:\n",
    "            data = nc.variables[variable][:].data[0,z_level, :, :][::-1, :]\n",
    "            data[data == -9999] = np.nan\n",
    "    gz.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyfortracc package\n",
    "import pyfortracc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "pyfortracc.plot_animation(path_files='input/*.gz', \n",
    "                          num_frames=10, min_val=-10, max_val=60,\n",
    "                          read_function=read_function,  cbar_title='dBZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parameters'></a>\n",
    "#### 4. Tracking Parameters\n",
    "\n",
    "For this example, we will track reflectivity clusters at multiple thresholds and sizes <br>Arbitrarily selecting thresholds of 20, 30 and 35 dBZ with clusters of 5,4 and 3 pixels <br>of minimum size. The segmentation operator will be >=, that is, the clusters will be <br>segmented in order of greatest equal for each threshold and the delta of the observations <br>is 12 minutes. The clustering algorithm in this example will be DBSCAN and the overlap <br>rate between clusters of consecutive times will be 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Name list dictionary of mandatory parameters\n",
    "name_list = {}\n",
    "name_list['input_path'] = 'input/' # path to the input data\n",
    "name_list['output_path'] = 'output/' # path to the output data\n",
    "name_list['timestamp_pattern'] = 'sbmn_cappi_%Y%m%d_%H%M.nc.gz' # timestamp file pattern\n",
    "name_list['thresholds'] = [20,30,35] # in dbz\n",
    "name_list['min_cluster_size'] = [3,3,3] # in number of points per cluster\n",
    "name_list['operator'] = '>=' # '>= - <=' or '=='\n",
    "name_list['delta_time'] = 12 # in minutes\n",
    "name_list['cluster_method'] = 'dbscan' # DBSCAN Clustering method\n",
    "name_list['min_overlap'] = 20 # Minimum overlap between clusters in percentage\n",
    "\n",
    "# Optional parameters, if not set, the algorithm will not use geospatial information\n",
    "name_list['lon_min'] = -62.1475 # Min longitude of data in degrees\n",
    "name_list['lon_max'] = -57.8461 # Max longitude of data in degrees\n",
    "name_list['lat_min'] = -5.3048 # Min latitude of data in degrees\n",
    "name_list['lat_max'] = -0.9912 # Max latitude of data in degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='workflow'></a>\n",
    "#### 5. Algorithm Workflow\n",
    "\n",
    "In this example we will use the modules separately, that is, each internal module of the pyForTraCC algorithm will be called individually.<br>\n",
    "The track workflow is divided into four modules:\n",
    "<ol>\n",
    "  <li><b>Feature detection</b>: Focuses on identifying distinct characteristics for precise tracking.\n",
    "  </li>\n",
    "  <li><b>Spatial Operations</b>: Involves spatial operations (intersection, union, difference, etc) between the features of consecutive time steps (t and t-1).\n",
    "  <li><b>Trajectory Linking</b>: Passing one by one the time steps, the algorithm link the features of consecutive time steps based on the association created in the previous step. The algorithm create a trajectory for each feature.\n",
    "  <li><b>Concatenate</b>:\n",
    "  </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Features Extraction\n",
    "Image processing strategies were combined with clustering and rasterization algorithms to achieve the results obtained by the extract_features function. The following sequence is present in the implementation of the algorithm:\n",
    "\n",
    "1. Read the file using the read_funcion.\n",
    "2. Image segmentation according to each threshold.\n",
    "3. Labeling of clusters. Two clustering options are implemented (DBSCAN and ndimage). \n",
    "5. Vectorization of the clusters using rasterio.features.shapes to acquire the boundary polygon and the centroid of the clusters.\n",
    "\n",
    "<span> <img src=\"./img/features.png\" style=\"height:320px;\" align=\"left\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Note: If you are running this notebook using MacOS, you may need to set parallel=False)\n",
    "pyfortracc.features_extraction(name_list, read_function, parallel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "features = sorted(glob.glob(name_list['output_path'] + '/track/processing/features/*.parquet'))\n",
    "features = pd.concat(pd.read_parquet(f) for f in features)\n",
    "display(features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Spatial operations \n",
    "\n",
    "Spatial junctions, fundamental in Geographic Information Systems (GIS) and spatial databases. And the Geopandas implementation simplifies the process by combining GeoDataframes stored in .parquet files based on their spatial relationships via the sjoin() method. This method performs various types of spatial joins such as overlays, within, contains.\n",
    "\n",
    "To demonstrate the basic functioning of the spatial operations mode, we will use as a base two consecutive times listed here as the variables cur_frame and prev_frame, which store information about the characteristics of the systems for each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.wkt import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the features parquet files\n",
    "cur_frame = pd.read_parquet('./output/track/processing/features/20140816_1012.parquet')\n",
    "prev_frame = pd.read_parquet('./output/track/processing/features/20140816_1000.parquet')\n",
    "cur_frame['geometry'] = cur_frame['geometry'].apply(loads)\n",
    "prev_frame['geometry'] = prev_frame['geometry'].apply(loads)\n",
    "# Convert to geo dataframes where column is geometry\n",
    "cur_frame = gpd.GeoDataFrame(cur_frame)\n",
    "prev_frame = gpd.GeoDataFrame(prev_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the features and see have a visual overlap between the geometries\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "cur_frame.boundary.plot(ax=ax, color='red', alpha=0.5, label='Current Frame')\n",
    "prev_frame.boundary.plot(ax=ax, color='blue', alpha=0.5, label='Previous Frame')\n",
    "plt.legend( loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the current frame\n",
    "display(cur_frame.head()[['timestamp','geometry']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the previous frame\n",
    "display(prev_frame.head()[['timestamp','geometry']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate one of the operations performed in the algorithm, below is the GeoPandas sjoin function that checks the overlaps between the two GeoDataframes. \n",
    "Note that the return of the function will be another GeoDataframe, however only the index and index_right columns are of interest to us, \n",
    "as in these columns we have the information we need to make the associations between the geometries of the consecutive time clusters.\n",
    "In addition to the overlap operation, there are several others that can be seen at:\n",
    "https://shapely.readthedocs.io/en/latest/manual.html#binary-predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the spatial join between the previous and current frame\n",
    "overlaps = gpd.sjoin(cur_frame, prev_frame, how='inner', predicate='overlaps')[['index_right']].reset_index()\n",
    "# Ex: index (cur_frame) 0 overlaps with index_right 0 (prev_franme)\n",
    "overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the spatial operations module also has additional vector extraction methods. These methods are covered in the work https://doi.org/10.3390/rs14215408\n",
    "\n",
    "To activate the methods just add the flags to the name_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add correction methods\n",
    "name_list['spl_correction'] = True # It is used to perform the correction at Splitting events\n",
    "name_list['mrg_correction'] = True # It is used to perform the correction at Merging events\n",
    "name_list['inc_correction'] = True # It is used to perform the correction using Inner Core vectors\n",
    "name_list['opt_correction'] = True # It is used to perform the correction using the Optical Flow method\n",
    "name_list['elp_correction'] = True # It is used to perform the correction using the Ellipse method\n",
    "name_list['validation'] = True # It is used to perform the validation of the correction methods\n",
    "name_list['scores'] = True # It is used to get the scores of the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the spatial_operations function (Note: If you are running this notebook using MacOS, you may need to set parallel=False)\n",
    "pyfortracc.spatial_operations(name_list, read_function, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the spatial parquet files\n",
    "spatial = sorted(glob.glob(name_list['output_path'] + '/track/processing/spatial/*.parquet'))\n",
    "spatial_df = pd.concat(pd.read_parquet(f) for f in spatial)\n",
    "display(spatial_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster Link\n",
    "\n",
    "The cluster connection module makes the association between consecutive time tables by associating the cluster indices that were identified by the spatial operations module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cluster_linking function\n",
    "pyfortracc.cluster_linking(name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we show how cluster link works. Set two spatial parquets from consecutive timestamps. And show the association between them based on indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read current and previous frames\n",
    "cur_frame = pd.read_parquet('output/track/processing/spatial/20140816_1212.parquet')\n",
    "prv_frame = pd.read_parquet('output/track/processing/spatial/20140816_1200.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Frame\n",
    "cur_frame.dropna(subset=['prev_idx']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous frame\n",
    "prv_frame.loc[cur_frame['prev_idx'].dropna().astype(int).values].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the linking results\n",
    "linked = sorted(glob.glob(name_list['output_path'] + '/track/processing/linked/*.parquet'))\n",
    "linking_df = pd.concat(pd.read_parquet(f) for f in linked)\n",
    "linking_df.loc[linking_df['trajectory'] != 'LINESTRING EMPTY'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate \n",
    "\n",
    "All features in one single file per timestamp, and create the Tracking Table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the features and spatial dataframes\n",
    "pyfortracc.concat(name_list, clean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Duration \n",
    "\n",
    "This module computes the duration of individual systems after the linking process. \n",
    "At this proccess, the algorithm calculates the duration of each system in minutes, and the location of genesis and lysis of each system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyfortracc.post_processing.compute_duration(name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tracking table is the generalized output entity of the algorithm, it is formed by the set of files (.parquet) that are located in the output directory of the same name ('output_path/trackingtable'). The information obtained in the tracking process is stored in a tabular format, and is organized according to the tracking time. Listed below are the names of the columns (output variables) and what they represent.\n",
    "\n",
    "- Each row of tracking table is related to a cluster at its corresponding threshold level. \n",
    "- The information spans from unique identifiers and descriptive statistics to geometric properties and temporal features. \n",
    "- The Tracking table structure provides a comprehensive view of grouped entities, facilitating analysis and understanding of patterns across different threshold levels.\n",
    "\n",
    "Tracking table columns:\n",
    "\n",
    "*   **timestamp** (datetime64[us]): Temporal information of cluster.\n",
    "*   **uid** (float64): Unique idetifier of cluster.\n",
    "*   **iuid** (float64): Internal Unique idetifier of cluster.\n",
    "*   **threshold_level** (int64): Level of threshold.\n",
    "*   **threshold** (float64): Specific threshold.\n",
    "*   **status** (object): Entity status (NEW, CONTINUOUS, SPLIT, MERGE, SPLIT/MERGE)\n",
    "*   **u_, v_** (float64): Vector components.\n",
    "*   **inside_clusters** (object): Number of inside clusters.\n",
    "*   **size** (int64): Cluster size in pixels.\n",
    "*   **min, mean, max, std** (float64): Descriptive statistics.\n",
    "*   **delta_time** (timedelta64[us]): Temporal variation.\n",
    "*   **file** (object): Associated file name.\n",
    "*   **array_y, array_x** (object): Cluster array coordinates.\n",
    "*   **vector_field** (object): Associated vector field.\n",
    "*   **trajectory** (object): Cluster's trajectory.\n",
    "*   **geometry** (object):  Boundary geometric representation of the cluster.\n",
    "*   **lifetime** (int64): Cluster lifespan in minutes.\n",
    "*   **duration** (int64): Cluster duration in minutes.\n",
    "*   **genesis** (int64): Cluster genesis, with genesis: 1, active: 0, and death: -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumed track Function\n",
    "\n",
    "Entery process we show above, is a workflow how the algorithm works. However, the track function is a simplified way to run the algorithm. And the way to use a simplified way is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track function\n",
    "pyfortracc.track(name_list, read_function, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tracking table is saved in the output path\n",
    "tracking_files = sorted(glob.glob(name_list['output_path'] + '/track/trackingtable/*.parquet'))\n",
    "tracking_table = pd.concat(pd.read_parquet(f) for f in tracking_files)\n",
    "display(tracking_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the results in tracking table\n",
    "\n",
    "At this exemple we will show the systems where max duration at the tracking table for the threshold 20.0 dBZ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get two maxlifetime clusters from the track_table\n",
    "maxlifetime = 2\n",
    "max_lifetimes = tracking_table.groupby('uid').size().nlargest(maxlifetime).index.values\n",
    "max_clusters = tracking_table[tracking_table['uid'].isin(max_lifetimes)]\n",
    "print('The clusters with the highest lifetime are the uids: {}'.format(max_lifetimes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the maxlifetime system in the tracking table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the tracking data for periods of time. Note: If the min and max value is a larger time interval, \n",
    "# the plot will be slower\n",
    "pyfortracc.plot_animation(read_function=read_function, # Read function\n",
    "                          name_list=name_list, # Name list dictionary\n",
    "                          uid_list=max_lifetimes.tolist(), # List of uids\n",
    "                          start_timestamp = max_clusters['timestamp'].min().strftime('%Y-%m-%d %H:%M:%S'), \n",
    "                          end_timestamp= max_clusters['timestamp'].max().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                          cbar_title='dBZ', # Colorbar title\n",
    "                          threshold_list=[20], # Threshold list\n",
    "                          trajectory=True, # Plot the trajectory\n",
    "                          smooth_trajectory=True, # Smooth the trajectory\n",
    "                          max_val=60, # Maximum value for the colorbar\n",
    "                          min_val=20, # Minimum value for the colorbar\n",
    "                          )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfortracc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
