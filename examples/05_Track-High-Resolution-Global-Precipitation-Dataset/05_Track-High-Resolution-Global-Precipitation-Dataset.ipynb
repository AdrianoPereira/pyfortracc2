{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"; span style=\"color:#336699\"><b><h2> Track High Resolution Global Precipitation </h2></b></div>\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "<br/>\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    Helvécio B. Leal Neto, <sup><a href=\"https://orcid.org/0000-0002-7526-2094\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    Alan J. P. Calheiros<sup><a href=\"https://orcid.org/0000-0002-7526-2094\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n",
    "    <br/><br/>\n",
    "    National Institute for Space Research (INPE)\n",
    "    <br/>\n",
    "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
    "    <br/><br/>\n",
    "    Contact: <a href=\"mailto:helvecio.neto@inpe.br\">helvecio.neto@inpe.br</a>, <a href=\"mailto:alan.calheiros@inpe.br\">alan.calheiros@inpe.br</a>\n",
    "    <br/><br/>\n",
    "    Last Update: Jun 16, 2024\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
    "<b>Abstract.</b> This Jupyter Notebook shows how to use a pyfortracc for track a high resolution dataset of global precipitation.\n",
    "</div>    \n",
    "<br/>\n",
    "<div style=\"text-align: justify;  margin-left: 15%; margin-right: 15%;font-size: 75%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n",
    "    <b>In this example, we will use pyfortracc to compute track of precipitating systems over the globe and explore the output data after the algorithm workflow.\n",
    "</b>\n",
    "    <div style=\"margin-left: 10px; margin-right: 10px; margin-top:10px\">\n",
    "      <p> Leal Neto, H.B.; Calheiros, A.J.P.;  pyForTraCC Algorithm. São José dos Campos, INPE, 2024. <a href=\"https://github.com/fortracc-project/pyfortracc\" target=\"_blank\"> Online </a>. </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule\n",
    " [1. Goals and the Data](#goals)<br>\n",
    " [2. Setup Env](#setup)<br>\n",
    " [3. Parameters: Name_list](#namelist)<br>\n",
    " [4. Track Visualization](#visualization)<br>\n",
    " [5. The Tracking Table](#tracktable)<br>\n",
    " [6. Post Processing](#post)<br>\n",
    " [7. Dask Processing](#dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='goals'></a>\n",
    "#### 1 . Goals - Track Example for Global Precipitation Data\n",
    "\n",
    "The data for this example is Self-Calibrating Multivariate Precipitation Retrieval ([SCaMPR](https://www.star.nesdis.noaa.gov/smcd/emb/ff/SCaMPR.php)) Rainfall Rate product RRQPE. The SCaMPR precipitation product is available every 10 minutes, over the Earth globe. Each precipitation data (netDF file) composite has 18000 x 6501 grid points, with high spatial resolution (2 km immediately below the satellite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ScamPR dataset\n",
    "!pip install --upgrade --no-cache-dir gdown &> /dev/null\n",
    "!gdown 'https://drive.google.com/uc?id=1CWei3m5xti6_JIoWzmMQj-NtGQ30YdlQ'\n",
    "!unzip -qq -o input.zip\n",
    "!rm -rf input.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read Function: \n",
    "The read_function is a Python definition function to read individual file and returns a 2D numpy array. Note: This function is mandatory for the Algorithm package, as it is used to read data passing a path as parameter. Below is the code of how the function should be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "def read_function(path):\n",
    "\treturn nc.Dataset(path)['RRQPE'][:].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_function('./input/RRQPE-INST-GLB_v1r1_blend_s202201010000000.nc')\n",
    "print('Shape of the data:', data.shape)\n",
    "print('Data type:', data.dtype)\n",
    "print('Min:', np.nanmin(data),'mm/h and', np.nanmax(data), 'mm/h')\n",
    "print('Example of the data:\\n', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='environment'></a>\n",
    "#### 2. Setting the environment\n",
    "\n",
    "Install package to environment and import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install the latest version of pyfortracc from the main branch\n",
    "# !pip install --upgrade git+https://github.com/fortracc-project/pyfortracc.git@main#egg=pyfortracc\n",
    "# Or import the local version of pyfortracc\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys\n",
    "# sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import library\n",
    "\n",
    "If everything is correct after installing the package, you can import the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfortracc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='namelist'></a>\n",
    "#### 3 . Track Parameters: Name List\n",
    "\n",
    "For this example we will track precipitation clusters with thresholds of 1mm/h and a minimum cluster size of 500 pixels, which corresponds to systems with an estimated size of 1000 km² (500 pixels * 2 km²/pixel). We will use the simple clustering method from the scipy 'ndimage.labels' library and enable the option to find clusters that are on the side edge (edges = True).\n",
    "\n",
    "The namelist is a python dictionary with the parameters for the correct track processing. Some of these parameters are mandatory and necessary for the complete tracking process, and others can be using to improve the performance of. If you need to consult these parameters you can use fortracc.default_parameters command. Or, open the file fortracc/default_parameters.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = {} # Set name_list dict\n",
    "name_list['input_path'] = 'input/' # path to the input data\n",
    "name_list['output_path'] = 'output/' # path to the output data\n",
    "name_list['thresholds'] = [1] # list of thresholds set as target (for this exemple each threshold are in mm/h)\n",
    "name_list['min_cluster_size'] = [500] # list of minimum size for cluster (in pixels)\n",
    "name_list['operator'] = '>=' # 'operator for segmentation process (>, >=, <, <=' or '==')\n",
    "name_list['timestamp_pattern'] = 'RRQPE-INST-GLB_v1r1_blend_s%Y%m%d%H%M%S0.nc' # timestamp file pattern format codes https://docs.python.org/3/library/datetime.html#format-codes\n",
    "name_list['delta_time'] = 10 # delta time interval (in minutes)\n",
    "name_list['cluster_method'] = 'ndimage' # Clustering method 'dbscan' (slower) or 'ndimage' (fast)\n",
    "name_list['edges'] = True # It is used to perform the cluster linking in the edges of the domain, is True if clusters cross the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tracking process\n",
    "pyfortracc.track(name_list, read_function, parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualization'></a>\n",
    "#### 4 . Track Visualization\n",
    "\n",
    "The visualization module is a fortracc utility designed to read the algorithm outputs and display the track and its other information in an easy way for the user.\n",
    "\n",
    "Before calling the visualization utility, it will be necessary to add some information to the name list. This information is useful for carrying out geospatial data conversions. The information being:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spatial information for use plot utility\n",
    "name_list['x_dim'] = 18000 # number of points in x\n",
    "name_list['y_dim'] = 6501 # number of points in y\n",
    "name_list['lon_min'] = -180.0 # Min longitude of data in degrees\n",
    "name_list['lon_max'] = 179.98 # Max longitude of data in degrees\n",
    "name_list['lat_min'] = -60.0 # Min latitude of data in degrees\n",
    "name_list['lat_max'] = 70.0 # Max latitude of data in degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view track image for a given time. The red polygons represent the objects identified according to the threshold chosen in the name_list\n",
    "pyfortracc.plot(name_list, read_function, '2022-01-01 00:50:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cross edges clusters\n",
    "pyfortracc.plot(name_list, read_function, '2022-01-01 00:50:00', cmap='turbo', info=True,info_cols=['uid'], uid_list=[463])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot animation utility passing a period between '2022-01-01 00:00:00' to '2022-01-01 01:00:00'\n",
    "pyfortracc.plot_animation(name_list, read_function,\n",
    "                          figsize=(20,5), # Figure size\n",
    "                          cmap='turbo', #Color map of plot\n",
    "                          start_stamp = '2022-01-01 00:00:00', # Timestamp start for animation\n",
    "                          end_stamp = '2022-01-01 01:00:00', # Timestamp end for animation\n",
    "                          uid_list=[463], # Uid list to filter\n",
    "                          info_cols=['uid','lifetime'],\n",
    "                          info=True, # Information box\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The utility's views also have configurations through their parameters.\n",
    "pyfortracc.plot(name_list, read_function, '2022-01-01 00:50:00',\n",
    "                cmap='turbo', # Color map of plot\n",
    "                zoom_region=[-60,-30,0,20], # Select a region [longitude_min, longitude_max, latitude_min, latitude_max]\n",
    "                info=True, # Show a box contain information from tracking table\n",
    "                info_cols=['uid','status','lifetime'], # Select column to show in information\n",
    "                uid_list=[272, 206], # Filter by uid\n",
    "                vector=True, vector_scale=20, vector_color='w', # Add vector direction (Vector scale is the size of arrow plot)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another visualization utility is plot_animation, which works similar to plot. However, in this module the user must spend a period that corresponds to tracked data for a tracking animation to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot animation utility passing a period between '2022-01-01 00:00:00' to '2022-01-01 01:00:00'\n",
    "pyfortracc.plot_animation(name_list, read_function,\n",
    "                          figsize=(15,5), # Figure size\n",
    "                          cmap='turbo', #Color map of plot\n",
    "                          start_stamp = '2022-01-01 00:00:00', # Timestamp start for animation\n",
    "                          end_stamp = '2022-01-01 01:00:00', # Timestamp end for animation\n",
    "                          uid_list=[272, 206], # Uid list to filter\n",
    "                          info=True, # Information box\n",
    "                          zoom_region=[-60,-30,0,20], #Zoom at region\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot animation utility passing a period between '2022-01-01 00:00:00' to '2022-01-01 01:00:00' and applying a zoom region\n",
    "pyfortracc.plot_animation(name_list, read_function,\n",
    "                          figsize=(15,5), # Figure size\n",
    "                          cmap='turbo', #Color map of plot\n",
    "                          start_stamp = '2022-01-01 00:00:00', # Timestamp start for animation\n",
    "                          end_stamp = '2022-01-01 01:00:00', # Timestamp end for animation\n",
    "                          uid_list=[272], # Uid list to filter\n",
    "                          info=True, # Information box\n",
    "                          info_cols=['uid'],\n",
    "                          zoom_region=[-50,-45,10,15], #Zoom at region\n",
    "                          traj_color='white' , traj_linewidth=5,\n",
    "                          centroid=True, centr_color='green', centr_size=10,\n",
    "                          vector=True, vector_scale=15, vector_color='w',\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tracktable'></a>\n",
    "#### 6 . Track Output (The tracking table)\n",
    "\n",
    "The tracking table is the generalized output entity of the algorithm, it is formed by the set of files (.parquet) that are located in the output directory of the same name ('output_path/trackingtable'). The information obtained in the tracking process is stored in a tabular format, and is organized according to the tracking time. Listed below are the names of the columns (output variables) and what they represent.\n",
    "\n",
    "- Each row in the tracking specific data related to a cluster at its corresponding threshold level. \n",
    "- The information spans from unique identifiers and descriptive statistics to geometric properties and temporal features. \n",
    "- The Tracking Table structure provides a comprehensive view of grouped entities, facilitating analysis and understanding of patterns across different threshold levels.\n",
    "\n",
    "Reading track files in the parquet format is efficiently done through the Dask library. Dask is a powerful Python library for parallel computing, designed to handle large datasets and facilitate distributed operations. By using Dask to read trace files in the parquet format, we can leverage its lazy computation capability, deferring operations until strictly necessary. This, coupled with Dask's ability to scale in distributed environments, makes reading and processing large trace datasets more efficient and accessible, providing agile and scalable analysis.\n",
    "\n",
    "Tracking table columns:\n",
    "\n",
    "*   **timestamp** (datetime64[us]): Temporal information of cluster.\n",
    "*   **uid** (float64): Unique idetifier of cluster.\n",
    "*   **iuid** (float64): Internal Unique idetifier of cluster.\n",
    "*   **threshold_level** (int64): Level of threshold.\n",
    "*   **threshold** (float64): Specific threshold.\n",
    "*   **lifetime** (timedelta64[ns]): Cluster lifespan.\n",
    "*   **status** (object): Entity status (NEW, CONTINUOUS, SPLIT, MERGE, SPLIT/MERGE)\n",
    "*   **u_, v_** (float64): Vector components.\n",
    "*   **inside_clusters** (object): Number of inside clusters.\n",
    "*   **size** (int64): Cluster size in pixels.\n",
    "*   **min, mean, max, std, Q1, Q2, Q3** (float64): Descriptive statistics.\n",
    "*   **delta_time** (timedelta64[us]): Temporal variation.\n",
    "*   **file** (object): Associated file name.\n",
    "*   **array_y, array_x** (object): Cluster array coordinates.\n",
    "*   **vector_field** (object): Associated vector field.\n",
    "*   **trajectory** (object): Cluster's trajectory.\n",
    "*   **geometry** (object):  Boundary geometric representation of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dask and distributed for parallel processing\n",
    "!pip install dask distributed --upgrade &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dask dataframe to read output tracking table\n",
    "import dask.dataframe as dd\n",
    "tracking_table = dd.read_parquet('output/track/trackingtable/*.parquet').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_table.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='post'></a>\n",
    "#### 7 . Post Processegin - Compute Duration\n",
    "\n",
    "To process overall cluster duration statistics, you can use the fortracc.post_processing.compute_duration post-processing method. This method groups clusters by uid and calculates the overall duration of events. With this module, a new column is created in the tracking_table called 'duration'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the post-processing utility to compute the duration of the objects\n",
    "pyfortracc.post_processing.compute_duration(namelist=name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying some spatial conventions to the tracking table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_table = dd.read_parquet('output/track/trackingtable/*.parquet').compute()\n",
    "tracking_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or Mount lifetime directly from tracking table\n",
    "tracking_table = tracking_table.set_index('timestamp')\n",
    "lifetime = tracking_table.groupby('uid')['lifetime'].max().to_frame()\n",
    "lifetime = lifetime.sort_values(by='lifetime', ascending=False)\n",
    "lifetime.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial degree to pixel conversion for column size\n",
    "PIXEL_SIZE = 2 # IN KM\n",
    "tracking_table['size'] = tracking_table['size'] * PIXEL_SIZE ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Clusters Exploration\n",
    "\n",
    "As a demonstration to explore the tracking results, you can select the tracked clusters individually. To do this, simply select the 'uid' and apply filters to the DataFrame in the same style as the Pandas library.\n",
    "\n",
    "In the example below we will select just one cluster to explore its tracking characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cluster uid\n",
    "CLUSTER_UID = 272\n",
    "filterd_cluster = tracking_table.loc[tracking_table['uid'] == CLUSTER_UID]\n",
    "filterd_cluster.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "f1 = filterd_cluster['size'].plot(ax=ax, marker='o', linestyle='dashed', color='r')\n",
    "f2 = ax.quiver(filterd_cluster.index, filterd_cluster['size'], filterd_cluster['u_'], filterd_cluster['v_'], color='b', \n",
    "             scale=0.5, scale_units='xy')\n",
    "ax.scatter(filterd_cluster.index, filterd_cluster['size'], s=1000, facecolors='none', edgecolors='black', alpha=0.5)\n",
    "ax.scatter(filterd_cluster.index, filterd_cluster['size'], marker='+', color='black', s=1000, alpha=0.5)\n",
    "# At each index create a box with the information of the cluster\n",
    "for i in range(len(filterd_cluster)):\n",
    "    ax.text(filterd_cluster.index[i] + pd.Timedelta(minutes=1),\n",
    "            filterd_cluster['size'][i] + 100,\n",
    "            f'status:{filterd_cluster[\"status\"][i]}\\n'\n",
    "            f'life:{filterd_cluster[\"lifetime\"][i].seconds//60} min\\n'\n",
    "            f'max:{filterd_cluster[\"max\"][i]:.2f}mm/h\\n'\n",
    "            f'mean:{filterd_cluster[\"mean\"][i]:.2f}mm/h\\n'\n",
    "            f'std:{filterd_cluster[\"std\"][i]:.2f}mm/h\\n'\n",
    "            f'Q1:{filterd_cluster[\"Q1\"][i]:.2f}mm/h\\n'\n",
    "            f'Q2:{filterd_cluster[\"Q2\"][i]:.2f}mm/h\\n'\n",
    "            f'Q3:{filterd_cluster[\"Q3\"][i]:.2f}mm/h\\n',\n",
    "            fontsize=6, color='black', zorder=10, weight=\"bold\",\n",
    "            bbox=dict(facecolor='white', alpha=0.3, edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title('Evolution of the cluster: {}'.format(CLUSTER_UID))\n",
    "ax.set_ylim(filterd_cluster['size'].min() - 1500, filterd_cluster['size'].max() + 1500)\n",
    "ax.set_xlim(filterd_cluster.index.min() - pd.Timedelta(minutes=15),\n",
    "            filterd_cluster.index.max() + pd.Timedelta(minutes=30))\n",
    "# Add grid at each 10 minutes\n",
    "ax.grid(True, which='both', axis='both', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "\n",
    "f1 = tracking_table.loc[tracking_table['uid'].isin(lifetime.head().index.values)][['uid','size']].groupby('uid')['size'].plot(ax=ax,\n",
    "                                                                                                                              legend=True,\n",
    "                                                                                                                              marker='o',\n",
    "                                                                                                                              linestyle='dashed');\n",
    "ax.set_ylabel('Size (Km²)')\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.set_title('Top 5 clusters with the longest lifetime')\n",
    "grid = ax.grid(True, which='both', axis='both', linestyle='--')\n",
    "# limit the x axis to the first 1000 minutes\n",
    "ax.set_xlim(tracking_table.index.min() -  pd.Timedelta(minutes=10), tracking_table.index.max() + pd.Timedelta(minutes=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dask'></a>\n",
    "#### 7 .  Using Dask Distributed Client\n",
    "The output data being parquet files, it is possible to use distributed processing by the dask library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tracking table\n",
    "tracking_table = dd.read_parquet('output/track/trackingtable/*.parquet').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import progress, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask Client to use in the next steps\n",
    "client = Client()\n",
    "def compute_dask(dask_df):\n",
    "    future = client.compute(dask_df)\n",
    "    progress(future, notebook=False) # Show progress\n",
    "    wait(future) # Wait for all tasks to finish\n",
    "    return future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tracking table\n",
    "tracking_table = dd.read_parquet('output/track/trackingtable/*.parquet',\n",
    "                                 columns=['uid','duration','iuid','threshold','size','mean','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the tracking table\n",
    "tracking_table['iuid'] = tracking_table['iuid'].fillna(tracking_table['uid'])\n",
    "tracking_table = tracking_table[tracking_table['duration'] >= pd.Timedelta('10 minutes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute main statistics for thresholds\n",
    "statistics_dsk = tracking_table.groupby(['iuid','threshold']).agg({'iuid': ['count'],\n",
    "                                                              'duration':['max'],\n",
    "                                                              'size': ['min', 'mean', 'max'],\n",
    "                                                              'mean': ['min', 'mean', 'max'],\n",
    "                                                              'max': ['min', 'mean', 'max'],\n",
    "                                                              })\n",
    "statistics_dsk.columns = ['_'.join(col).strip() for col in statistics_dsk.columns.values]\n",
    "statistics_df = compute_dask(statistics_dsk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df.groupby('threshold')['duration_max'].agg(['count','mean','std','min','max'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrotrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
